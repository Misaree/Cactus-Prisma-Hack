# -*- coding: utf-8 -*-
"""text_summarisation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17sVHa-9kwzLDfyovcPzwe2tddBHxMBOf
"""

!pip install requests
!pip install huggingface_hub
from huggingface_hub import login
login("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")
!pip install transformers


from transformers import pipeline, AutoTokenizer

# Load the BART summarization pipeline and tokenizer
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
tokenizer = AutoTokenizer.from_pretrained("facebook/bart-large-cnn")

# Function to split text into chunks
def split_text_into_chunks(text, chunk_size=500):
    words = text.split()
    chunks = [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

# Ensure text is not empty
if pdf_text.strip():
    text_chunks = split_text_into_chunks(pdf_text, chunk_size=500)  # Adjust chunk size if needed

    summaries = []
    buffer = ""  # Buffer to store small chunks

    for chunk in text_chunks:
        chunk = buffer + " " + chunk  # Add leftover text from the previous small chunk
        buffer = ""  # Reset buffer

        # Skip if too small (less than 10 words)
        if len(chunk.split()) < 10:
            buffer = chunk  # Store for next chunk
            continue

        try:
            # Tokenize the chunk
            tokenized_chunk = tokenizer(chunk, truncation=True, max_length=1024, return_tensors="pt")
            num_words = len(chunk.split())

            # Dynamic min/max length to avoid index errors
            max_len = min(150, num_words // 2) if num_words > 40 else num_words
            min_len = min(30, num_words // 4) if num_words > 20 else 5  # Avoid out-of-range error

            # Summarize
            summary = summarizer(chunk, max_length=max_len, min_length=min_len, do_sample=False)
            summaries.append(summary[0]['summary_text'])

        except IndexError:  # Catch the out-of-range error
            print(f"IndexError for chunk: {chunk[:100]}... (Reducing min_length and retrying)")
            try:
                summary = summarizer(chunk, max_length=50, min_length=2, do_sample=False)
                summaries.append(summary[0]['summary_text'])
            except Exception as e:
                print(f"Failed again: {str(e)}")
                buffer = chunk  # Store it for later processing

    # Combine the summaries
    final_summary = " ".join(summaries)
    print(final_summary)
else:
    print("Error: Extracted text is empty!")